## Assignment Feedback & Engineering Insights  

Working on this assignment revealed several interesting challenges that allowed me to showcase my data analysis, troubleshooting, and problem-solving skills – qualities I honed during my time with the Department of Defense (DoD) and in the Big Auto industry.  

### Optimizing Data Retrieval  
Initially every time I ran my code, I had to make HTTP requests to fetch the data. This was inefficient due to the I/O overhead – especially for older and larger datasets to request (i.e., title 7 from 3/27/2020), where the server wasn’t always cooperative. To improve my workflow, I made one final API call to download the entire dataset locally for the required current data and historical data. This allowed me to test and iterate on my code much more efficiently.  

### Handling Historical Data Variability  
While the current date’s API endpoint worked as expected, querying older, historical data led to intermittent HTTP 504 timeouts. I addressed this by increasing the timeout value of the request and implementing a fallback mechanism. With this approach, if a full title download failed, the system would try retrieving data in smaller, manageable chapter chunks, ensuring the process remained robust even when the server was less stable.  

### Leveraging the 'Children' Subset in the Agency JSON  
During my initial analysis, I noticed some word count anomalies – for instance, the Department of the Treasury showed only 69k words and the Department of Agriculture 184k words, which seemed unusually low compared to other agencies. Digging deeper, I discovered that I had overlooked the `children` subset in the top-level agency JSON, which contained additional, valuable embedded CFR references. Integrating these extra references into my analysis significantly improved the accuracy of the word counts, leading to a more comprehensive understanding of each agency’s regulatory data.  

### Focused Word Count Analysis for Meaningful Insights  
For the word count analysis, I plotted regulation words for all 153 agencies to provide a complete view of the regulatory landscape. To make the data more actionable and user-friendly, I also created a Top 20 leaderboard to highlight the agencies with the largest regulation footprints. This approach helps focus attention on the "biggest fish" first — where the volume of text is greatest and the potential impact of streamlining or de-regulation could be most significant. It also provides a natural starting point for policymakers and analysts to prioritize review efforts.  


### Historical Change Analysis  
For the historical analysis, I anchored the comparison on today's date (March 27, 2025) and looked back exactly five years to 2020 to assess how the regulation text volume has evolved. In 2020, the text volume was **76,510,816 words**, and by 2025 it increased to **95,958,680 words** – an increase of **19,447,864 words**, or roughly a **+25.4% growth**. This significant rise not only highlights the growing complexity of federal regulations but also signals a trend that could hinder entrepreneurship. As regulatory burdens expand, potential innovators and business owners may find it increasingly challenging to navigate compliance requirements, ultimately making America a less attractive destination for new ventures.  

### Checksum Analysis  
For my checksum analysis, not only did I compute a checksum for the regulation text of the current dataset, but I also applied it to the historical dataset. This allowed me to compare the 2020 snapshot with the current 2025 data, revealing that **99 out of 153 agencies (64.7%)** have experienced changes in their regulation text. This approach demonstrates my ability to integrate multiple analytical tasks and extract meaningful insights from complex datasets.  

### Custom Metrics for Actionable Insights  
I developed a custom metric – the **Regulatory Density Score** – to measure the average word count per CFR reference. This score reveals how verbose some sections can be, much like lengthy "Terms and Conditions" that cause users to skim over the fine print. To illustrate this, I plotted the data and created a **Top 20 Leaderboard** showcasing the agencies with the highest density scores, clearly indicating where streamlining could have the most impact. Overall, the **average Regulatory Density Score** across agencies is **627,180.92 words per reference**.  

### Real-World Experience and Continued Improvement  
I have firsthand experience working in the DoD, both as an engineer and as a team lead. I led a $32M System Test & Evaluation proposal for a next-gen digital radar system, where I prepared detailed budget estimates that influenced MDA priorities and Congressional decisions. I've seen the inefficiencies – bloated travel budgets, overpriced bill of materials, poor integration workflows – and, while not trying to be a whistleblower, I've also witnessed padded markups in proposals, smoothing of data to make submissions appear overly optimistic, and chaotic auditing processes when it comes to tracking government-owned equipment. These experiences give me a practical lens on where bureaucracy can mask inefficiency, and reinforce the value of using clean, data-driven methods to cut through the noise and drive real accountability. I know how valuable this kind of data-driven approach can be in highlighting where regulation (or spending) isn’t proportionate to value.  

This project was a natural fit for me – I enjoy working across APIs, wrangling data, visualizing insights, and asking uncomfortable questions about complexity and scale. While the assignment recommended keeping the effort to under four hours, I want to acknowledge that I went well beyond that. Not out of obligation—but because I genuinely enjoyed it. The challenge, the data, the mission - it all resonated with me. This felt like more than just a take-home project. It felt like the kind of work that matters. I see this opportunity as something special and rare. To me, the DOGE team feels like a modern-day, tech-forward version of the Founding Fathers - bringing transparency, accountability, and creativity into government through data. That’s something I want to be a part of. I put a lot of time and care into this submission because I believe in the impact this kind of work can have. And even after submitting, I plan to keep iterating and improving this project in my free time—because it’s just that cool.  

I believe these solutions reflect my deep expertise in data analysis, optimization, and scalable systems, all of which are critical in driving efficiency and informed decision-making in government settings. I appreciate the opportunity to work on this assignment and look forward to contributing my experience to the team.  
